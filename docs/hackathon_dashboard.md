# Hackathon Dashboard

## **Описание задачи**

 Дашборд должен отображать аналитику про модель и бота в целом на основе:
 * логов запросов пользователей
 * базовых метрик, файл [`metrics/`](../metrics.py).
 * (⭐) выдумать метрику для оценки следующей ситуации с моделью: в контекст пришли несколько кусочков документов, но модель начала путать понятия и итоговый ответ получился очень убедительным с точки зрения пользователя, который не знаком с правилами ВШЭ, однако по своей сути неверным :()

## **Немного важных фактов про нашу модель и задание в целом**
То, что перечислено в  задачах (ниже) - это, конечно, круто и важно, но советуем вам начать с другого.
На вопрос "Кто будет пользоваться дашбордом?", ответ - "Бизнес". И бизнесу важнее понимать производительность бота (модели, rag-pipeline), нежели чем просто распределение ответов по кампусам. Нам бы очень хотелось уметь дектектировать ошибку модели. А какие могут быть ошибки у RAG-pipeline? Давайте попробуем классифицировать, например:
1. В модель пришли некорректные контексты:
   - они не подходят по смыслу к вопросу
   - они маленького размера
   - из них непонятен смысл документа откуда взяли (те не кусок документа, а набор рандомных слов, несвязных)
     и тд
2. В модель пришли корректные контексты, но она:
   - триггерится цензором на некоторые слова
   - выдает не обдуманный ответ, а мешанину из контекстов
   - выдает очень уверенный ответ, но плохой
   и тд
Как это посчитать численно? Как показать, что бот неэффективен? Именно на этот вопрос вам и предстоит ответить и представить реализации ваших идей на дашборде :)

## **Задачи**
1. **Визуализация пользовательских метрик:**
   - Распределение по кампусам 
    ```python
    campuses = ["Москва", "Нижний Новгород", "Санкт-Петербург", "Пермь"]
    ```

   - Разбивка по уровням образования 
    ```python
    education_levels = ["бакалавриат", "магистратура", "специалитет", "аспирантура"]
    ```

2. **Визуализация метрик вопросов:**
   - Категории вопросов 
    ```python
    question_categories = [
    "Деньги",
    "Учебный процесс",
    "Практическая подготовка",
    "ГИА",
    "Траектории обучения",
    "Английский язык",
    "Цифровые компетенции",
    "Перемещения студентов / Изменения статусов студентов",
    "Онлайн-обучение",
    "Цифровые системы",
    "Обратная связь",
    "Дополнительное образование",
    "Безопасность",
    "Наука",
    "Социальные вопросы",
    "ВУЦ",
    "Общежития",
    "ОВЗ",
    "Внеучебка",
    "Выпускникам",
    "Другое"
    ]
    ```

3. **История чатов:**
   - Анализ частоты повторяющихся вопросов. (близких по смыслу)
   - Подсчет базовых метрик ([`metrics/`](../metrics.py)) и ваших, которые сочтете нужными.

4. **Общая производительность:**
   - Среднее время обработки вопросов. ("Время ответа модели" в логах модели)
   - Частота пустых параметров chat_history (это значит что пользователю понравился ответ модели и он не стал переспрашивать)
   - Частота непустых параметров chat_history (это значит что пользователю не понравился ответ модели и он решил уточнить свой вопрос)

## **Входные данные**

Логи модели выглядят следующим образом:
```json
{
    "Выбранная роль": "Студент",
    "Кампус": "Нижний Новгород",
    "Уровень образования": "Бакалавриат",
    "Категория вопроса": "Учеба",
    "Время ответа модели": 3.294378,
    "user_filters": [
        "Нижний Новгород",
        "бакалавриат"
    ],
    "question_filters": [
        "Учебный процесс",
        "Практическая подготовка"
    ],
    "chat_history": {
        "old_contexts": [
            "Пример текста контекста"
        ],
        "old_questions": [
            "Когда пересдача?"
        ],
        "old_answers": [
            "Пересдача пройдет в сентябре."
        ]
    }
}
```

**Важно:** Иногда `chat_history` может быть пустым





